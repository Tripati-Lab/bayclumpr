[{"path":"https://tripati-lab.github.io/bayclumpr/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 bayclumpr authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/articles/bayclumpr.html","id":"performing-calibrations-using-bayclumpr","dir":"Articles","previous_headings":"","what":"Performing calibrations using bayclumpr","title":"bayclumpr","text":"First, need data work . can use bayclumpr generate simulated datasets uncertainty values described Roman-Palacios et al. (2022). example, simulate 50 observations low-eror scenario. Note functions bayclumpr expect users provide uncertainty terms standard deviation. resulting dataset stored ds object. Now, let’s start fitting different models simulated dataset. instance, let’s fit Deming regression model using cal.deming function bayclumpr: Alternatively, can fit unweighted weighted OLS regression using cal.ols cal.wols functions, respectively: York regression models also implemented bayclumpr: Finally, bayclumpr implements three types Bayesian linear models used calibrations temperature reconstructions. Let’s fit three models using cal.bayesian function: results stored BayesCal object corresponds stan objects summarizing posterior distributions parameters:","code":"ds <- cal.dataset(error = \"S1\", nobs = 50) head(ds) #>      x_TRUE Temperature    TempError    y_TRUE      D47error       D47 Material #> 1 10.076133   10.089943  0.013809939 0.6475262 -0.0032283733 0.6442978        1 #> 2 11.414949   11.399569 -0.015379377 0.6841481  0.0065876134 0.6907357        1 #> 3 12.517576   12.522651  0.005074617 0.7430624  0.0012176807 0.7442800        1 #> 4  9.695736    9.662728 -0.033008011 0.6333012  0.0021347308 0.6354360        1 #> 5 12.391566   12.364749 -0.026817078 0.7379670  0.0027211068 0.7406881        1 #> 6 12.060248   12.051630 -0.008617473 0.7206252  0.0005650349 0.7211903        1 cal.deming(data = ds, replicates = 10) #>        alpha       beta #> 1  0.2440497 0.03906877 #> 2  0.2614754 0.03670698 #> 3  0.2760968 0.03574009 #> 4  0.2419661 0.03857797 #> 5  0.2752911 0.03585697 #> 6  0.2526154 0.03764163 #> 7  0.2497403 0.03784485 #> 8  0.2505127 0.03785190 #> 9  0.2590315 0.03708834 #> 10 0.2057244 0.04158558 cal.ols(data = ds, replicates = 10) #>        alpha       beta #> 1  0.2839352 0.03554307 #> 2  0.2872762 0.03527608 #> 3  0.2733101 0.03642586 #> 4  0.2640842 0.03709674 #> 5  0.2652946 0.03701428 #> 6  0.2668595 0.03679151 #> 7  0.2660233 0.03687870 #> 8  0.2826984 0.03542715 #> 9  0.2693398 0.03663859 #> 10 0.2850873 0.03507551 cal.wols(data = ds, replicates = 10) #>        alpha       beta #> 1  0.2557905 0.03789321 #> 2  0.2734807 0.03632590 #> 3  0.2716440 0.03643221 #> 4  0.2711929 0.03621119 #> 5  0.2743809 0.03625556 #> 6  0.2716777 0.03629290 #> 7  0.2765445 0.03604140 #> 8  0.2747726 0.03619440 #> 9  0.2710448 0.03650991 #> 10 0.2699076 0.03685035 cal.york(data = ds, replicates = 10) #>        alpha       beta #> 1  0.2502012 0.03822585 #> 2  0.2417208 0.03889872 #> 3  0.2430982 0.03835195 #> 4  0.2689329 0.03625636 #> 5  0.2714125 0.03611063 #> 6  0.2638400 0.03685362 #> 7  0.2897728 0.03520313 #> 8  0.2755461 0.03600768 #> 9  0.2630668 0.03715828 #> 10 0.2678943 0.03654801 BayesCal <- cal.bayesian(calibrationData = ds, numSavedSteps = 3000, priors = \"Weak\", MC = FALSE) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 3.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup) #> Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup) #> Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup) #> Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup) #> Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup) #> Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling) #> Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling) #> Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling) #> Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling) #> Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling) #> Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling) #> Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 1.256 seconds (Warm-up) #> Chain 1:                1.193 seconds (Sampling) #> Chain 1:                2.449 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.1e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup) #> Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup) #> Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup) #> Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup) #> Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup) #> Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling) #> Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling) #> Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling) #> Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling) #> Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling) #> Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling) #> Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 1.04 seconds (Warm-up) #> Chain 2:                0.701 seconds (Sampling) #> Chain 2:                1.741 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup) #> Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup) #> Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup) #> Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup) #> Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup) #> Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling) #> Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling) #> Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling) #> Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling) #> Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling) #> Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling) #> Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.295 seconds (Warm-up) #> Chain 1:                0.284 seconds (Sampling) #> Chain 1:                0.579 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup) #> Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup) #> Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup) #> Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup) #> Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup) #> Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling) #> Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling) #> Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling) #> Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling) #> Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling) #> Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling) #> Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.351 seconds (Warm-up) #> Chain 2:                0.286 seconds (Sampling) #> Chain 2:                0.637 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.8e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup) #> Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup) #> Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup) #> Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup) #> Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup) #> Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling) #> Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling) #> Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling) #> Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling) #> Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling) #> Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling) #> Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.454 seconds (Warm-up) #> Chain 1:                0.323 seconds (Sampling) #> Chain 1:                0.777 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 8e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup) #> Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup) #> Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup) #> Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup) #> Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup) #> Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling) #> Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling) #> Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling) #> Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling) #> Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling) #> Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling) #> Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.397 seconds (Warm-up) #> Chain 2:                0.338 seconds (Sampling) #> Chain 2:                0.735 seconds (Total) #> Chain 2: BayesCal"},{"path":"https://tripati-lab.github.io/bayclumpr/articles/bayclumpr.html","id":"reconstructing-temperatures-in-bayclumpr","dir":"Articles","previous_headings":"","what":"Reconstructing temperatures in bayclumpr","title":"bayclumpr","text":"bayclumpr implements two functions perform temperature reconstructions frequentist (rec.clumped) Bayesian frameworks (rec.bayesian). Let’s review functions work generating synthetic dataset two samples. calibration step, bayclumpr expects uncertainty (D47error) expressed terms standard deviation. Note recData object generated includes smallest number columns needed perform reconstructions bayclumpr. point, need either specify distribution parameter estimates calibration step. instance, let’s assume interested reconstructing temperatures recData OLS model. First, run calibration analyses: point, can use rec.clumped reconstruct temperatures based reconstruction dataset (recData argument) observed calibration object (obCal argument): resulting object includes information template reconstruction dataset also information reconstructed temperature associated uncertainty (1 SD). Let’s now perform reconstructions Bayesian framework. , need parameter estimates derived calibration step (see BayesCal created ). perform reconstructions single Bayesian models equivalent OLS fit Bayesian framework (BayesCal$BLM1_fit_NoErrors). associated reconstructions Bayesian model shown :","code":"recData <- data.frame(Sample = paste(\"Sample\", 1:9),                       D47 = rep(c(0.6, 0.7, 0.8), 3),                       D47error = c(rep(0.005,3), rep(0.01,3), rep(0.02,3)),                       N = rep(2, 9),                       Material = rep(1, 9)) paramdist <- cal.ols(data = ds, replicates = 10) rec.clumped(recData = recData, obCal = paramdist) #>     Sample D47 D47error  meanTemp    error #> 1 Sample 1 0.6    0.005  59.79108 2.509437 #> 2 Sample 2 0.7    0.005  18.30648 1.687880 #> 3 Sample 3 0.8    0.005 -10.74432 1.233827 #> 4 Sample 4 0.6    0.010  59.79108 4.962974 #> 5 Sample 5 0.7    0.010  18.30648 3.346772 #> 6 Sample 6 0.8    0.010 -10.74432 2.450411 #> 7 Sample 7 0.6    0.020  59.79108 9.710425 #> 8 Sample 8 0.7    0.020  18.30648 6.580837 #> 9 Sample 9 0.8    0.020 -10.74432 4.833432 PredsBay <- rec.bayesian(calModel = BayesCal$BLM1_fit_NoErrors, recData = recData, iter = 1000, postcalsamples = 100, MC = FALSE) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 0.00013 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.3 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup) #> Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup) #> Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup) #> Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling) #> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.677 seconds (Warm-up) #> Chain 1:                0.541 seconds (Sampling) #> Chain 1:                1.218 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7.5e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.75 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup) #> Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup) #> Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup) #> Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.666 seconds (Warm-up) #> Chain 2:                0.539 seconds (Sampling) #> Chain 2:                1.205 seconds (Total) #> Chain 2: PredsBay #>     Sample D47 D47error  meanTemp     error #> 1 Sample 1 0.6    0.005  59.96228 0.8033461 #> 2 Sample 2 0.7    0.005  18.58978 0.5165987 #> 3 Sample 3 0.8    0.005 -10.42620 0.3780554 #> 4 Sample 4 0.6    0.010  59.97027 0.8644890 #> 5 Sample 5 0.7    0.010  18.56865 0.5942328 #> 6 Sample 6 0.8    0.010 -10.43481 0.4013505 #> 7 Sample 7 0.6    0.020  59.98923 1.3046313 #> 8 Sample 8 0.7    0.020  18.59688 0.8196340 #> 9 Sample 9 0.8    0.020 -10.42618 0.6088671"},{"path":"https://tripati-lab.github.io/bayclumpr/articles/bayclumpr.html","id":"including-a-co-variate","dir":"Articles","previous_headings":"","what":"Including a co-variate","title":"bayclumpr","text":"Now, instead single predictor, use second column (.e., Ion) ’s error (.e., IonError) predictors. Let’s first create calibration dataset. Note using super informative values Ion error. just need include two new columns dataset: point, can fit regression model accounts uncertainty two predictors , can simply run reconstruction based synthetic reconstruction dataset following structure: reconstruction can performed follows: resulting reconstructions follow:","code":"ds <- cal.dataset(error = \"S1\", nobs = 50) ds$Ion <- rnorm(nrow(ds)) ds$IonError <- rnorm(nrow(ds), 2) ionmodel <- cal.ion.bayesian(calibrationData = ds) ionmodel #> $BLM1_fit #> Inference for Stan model: anon_model. #> 2 chains, each with iter=2500; warmup=1000; thin=1;  #> post-warmup draws per chain=1500, total post-warmup draws=3000. #>  #>              mean se_mean   sd  2.5%   25%   50%    75%  97.5% n_eff Rhat #> alpha        0.27    0.00 0.01  0.24  0.26  0.27   0.28   0.30  5383    1 #> beta         0.04    0.00 0.00  0.03  0.04  0.04   0.04   0.04  5304    1 #> gamma        0.00    0.00 0.00 -0.01  0.00  0.00   0.00   0.01  1782    1 #> sigma        0.01    0.00 0.00  0.01  0.01  0.01   0.01   0.02  3192    1 #> log_lik[1]   3.31    0.00 0.16  2.96  3.23  3.33   3.41   3.57  2762    1 #> log_lik[2]   3.34    0.00 0.13  3.06  3.26  3.34   3.42   3.57  3144    1 #> log_lik[3]   2.64    0.01 0.35  1.80  2.46  2.69   2.87   3.21  2593    1 #> log_lik[4]   3.08    0.01 0.31  2.40  2.93  3.12   3.28   3.53  2343    1 #> log_lik[5]   2.58    0.01 0.34  1.88  2.39  2.59   2.80   3.24  2765    1 #> log_lik[6]   3.03    0.00 0.27  2.37  2.92  3.07   3.19   3.43  2963    1 #> log_lik[7]   3.31    0.00 0.17  2.96  3.24  3.32   3.41   3.57  2842    1 #> log_lik[8]   1.90    0.01 0.70  0.37  1.49  1.95   2.35   3.17  2613    1 #> log_lik[9]   2.84    0.01 0.39  1.85  2.65  2.92   3.10   3.38  2985    1 #> log_lik[10]  2.85    0.01 0.36  2.03  2.63  2.88   3.10   3.45  2751    1 #> log_lik[11]  3.21    0.00 0.18  2.79  3.12  3.23   3.33   3.52  3234    1 #> log_lik[12]  3.33    0.00 0.16  2.99  3.25  3.34   3.42   3.58  3124    1 #> log_lik[13]  3.36    0.00 0.12  3.11  3.28  3.36   3.43   3.59  2577    1 #> log_lik[14]  1.75    0.01 0.54  0.64  1.41  1.76   2.09   2.86  3039    1 #> log_lik[15]  3.33    0.00 0.13  3.07  3.26  3.34   3.42   3.56  2489    1 #> log_lik[16]  3.33    0.00 0.14  3.02  3.26  3.35   3.42   3.57  3639    1 #> log_lik[17]  3.31    0.00 0.15  2.99  3.23  3.32   3.40   3.56  3293    1 #> log_lik[18]  3.34    0.00 0.15  3.01  3.26  3.35   3.43   3.58  2754    1 #> log_lik[19]  3.14    0.01 0.26  2.47  3.03  3.19   3.31   3.50  2655    1 #> log_lik[20]  3.32    0.00 0.15  3.02  3.24  3.33   3.41   3.56  2904    1 #> log_lik[21]  2.60    0.01 0.34  1.85  2.40  2.62   2.81   3.23  3521    1 #> log_lik[22]  3.27    0.00 0.19  2.80  3.19  3.30   3.38   3.55  3497    1 #> log_lik[23]  3.25    0.00 0.16  2.92  3.16  3.26   3.35   3.54  2054    1 #> log_lik[24]  3.21    0.00 0.22  2.69  3.10  3.24   3.35   3.53  3166    1 #> log_lik[25]  3.22    0.00 0.18  2.85  3.12  3.23   3.33   3.54  2334    1 #> log_lik[26]  3.28    0.00 0.19  2.81  3.19  3.30   3.40   3.57  2269    1 #> log_lik[27]  3.02    0.01 0.27  2.43  2.86  3.05   3.20   3.46  2753    1 #> log_lik[28]  3.04    0.00 0.26  2.42  2.91  3.07   3.21   3.45  3017    1 #> log_lik[29]  2.01    0.01 0.51  1.02  1.70  2.02   2.31   3.03  3056    1 #> log_lik[30]  2.92    0.00 0.30  2.22  2.77  2.95   3.12   3.39  4511    1 #> log_lik[31]  3.13    0.00 0.23  2.60  3.01  3.16   3.29   3.51  2685    1 #> log_lik[32]  3.35    0.00 0.13  3.09  3.28  3.36   3.43   3.58  2885    1 #> log_lik[33]  2.18    0.01 0.47  1.14  1.92  2.21   2.48   2.98  3109    1 #> log_lik[34]  2.23    0.01 0.55  0.88  1.96  2.30   2.59   3.07  2822    1 #> log_lik[35]  3.36    0.00 0.13  3.11  3.29  3.37   3.44   3.59  3029    1 #> log_lik[36]  1.04    0.01 0.72 -0.59  0.62  1.08   1.52   2.28  3586    1 #> log_lik[37]  2.68    0.01 0.41  1.76  2.43  2.72   2.97   3.37  2662    1 #> log_lik[38]  3.18    0.00 0.20  2.67  3.09  3.21   3.30   3.50  2984    1 #> log_lik[39]  3.08    0.00 0.23  2.54  2.95  3.11   3.24   3.47  3202    1 #> log_lik[40]  3.10    0.00 0.23  2.55  2.99  3.13   3.25   3.47  2958    1 #> log_lik[41]  2.71    0.01 0.33  1.97  2.54  2.74   2.92   3.28  3756    1 #> log_lik[42]  1.91    0.01 0.55  0.79  1.57  1.92   2.24   3.01  2815    1 #> log_lik[43]  3.18    0.00 0.24  2.60  3.07  3.23   3.35   3.53  2390    1 #> log_lik[44]  3.35    0.00 0.14  3.05  3.27  3.36   3.43   3.58  3277    1 #> log_lik[45]  2.22    0.01 0.45  1.21  1.99  2.27   2.51   2.98  3109    1 #> log_lik[46]  3.30    0.00 0.18  2.84  3.22  3.32   3.41   3.58  2087    1 #> log_lik[47]  2.84    0.01 0.28  2.19  2.70  2.86   3.00   3.32  2530    1 #> log_lik[48]  3.24    0.00 0.19  2.79  3.15  3.26   3.36   3.55  2294    1 #> log_lik[49]  2.15    0.01 0.45  1.15  1.91  2.18   2.43   2.95  2891    1 #> log_lik[50]  3.20    0.00 0.26  2.49  3.11  3.25   3.35   3.51  2947    1 #> lp__        99.41    0.21 7.37 83.88 94.58 99.66 104.60 113.07  1199    1 #>  #> Samples were drawn using NUTS(diag_e) at Tue Jul  8 13:03:17 2025. #> For each parameter, n_eff is a crude measure of effective sample size, #> and Rhat is the potential scale reduction factor on split chains (at  #> convergence, Rhat=1). recData <- data.frame(Sample = paste(\"Sample\", 1:9),                       D47 = rep(c(0.6, 0.7, 0.8), 3),                       D47error = c(rep(0.005,3), rep(0.01,3), rep(0.02,3)),                       Ion = rep(c(0.6, 0.7, 0.8), 3),                       IonError = c(rep(0.005,3), rep(0.01,3), rep(0.02,3)),                       N = rep(2, 9),                       Material = rep(1, 9)) PredsBay <- rec.ion.bayesian(calModel = ionmodel[[1]],                          recData = recData,                          iter = 1000,                          postcalsamples = 100, MC = FALSE) #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 0.000192 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.92 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup) #> Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup) #> Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup) #> Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling) #> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 9.044 seconds (Warm-up) #> Chain 1:                1.799 seconds (Sampling) #> Chain 1:                10.843 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 9.6e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup) #> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup) #> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup) #> Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup) #> Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup) #> Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup) #> Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling) #> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling) #> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling) #> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling) #> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling) #> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 7.515 seconds (Warm-up) #> Chain 2:                1.802 seconds (Sampling) #> Chain 2:                9.317 seconds (Total) #> Chain 2: PredsBay #>     Sample D47 D47error  meanTemp     error #> 1 Sample 1 0.6    0.005  60.11940  7.608069 #> 2 Sample 2 0.7    0.005  18.36948  4.989418 #> 3 Sample 3 0.8    0.005 -10.77794  3.718707 #> 4 Sample 4 0.6    0.010  60.20635  8.824740 #> 5 Sample 5 0.7    0.010  18.38667  5.794015 #> 6 Sample 6 0.8    0.010 -10.75082  4.308152 #> 7 Sample 7 0.6    0.020  60.56448 12.611447 #> 8 Sample 8 0.7    0.020  18.58468  8.292557 #> 9 Sample 9 0.8    0.020 -10.64488  6.115418"},{"path":"https://tripati-lab.github.io/bayclumpr/articles/bayclumpr.html","id":"outlook","dir":"Articles","previous_headings":"","what":"Outlook","title":"bayclumpr","text":"reviewed fundamental aspects using bayclumpr. advances analyses involving alternative priors Bayesian models option.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Cristian Roman Palacios. Author, maintainer. Hannah M. Carroll. Author. Aradhna Tripati. Author.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Roman Palacios C, Carroll H, Tripati (2025). bayclumpr: Bayesian Analysis Clumped Isotope Datasets. R package version 0.1.2, https://bayclump.tripatilab.epss.ucla.edu/.","code":"@Manual{,   title = {bayclumpr: Bayesian Analysis of Clumped Isotope Datasets},   author = {Cristian {Roman Palacios} and Hannah M. Carroll and Aradhna Tripati},   year = {2025},   note = {R package version 0.1.2},   url = {https://bayclump.tripatilab.epss.ucla.edu/}, }"},{"path":[]},{"path":[]},{"path":"https://tripati-lab.github.io/bayclumpr/index.html","id":"what-is-bayclumpr","dir":"","previous_headings":"","what":"What is bayclumpr?","title":"Bayesian Analysis of Clumped Isotope Datasets","text":"support use Bayesian models analytical framework developed Román-Palacios et al. (2022) clumped isotope calibration temperature reconstructions, facilitate comparisons Bayesian classical models, present self-contained R package associated Shiny Dashboard application, bayclumpr BayClump, respectively. bayclumpr (BayClump) fits frequentist Bayesian linear regressions calibration datasets performs temperature reconstructions frameworks. can find details use bayclumpr website.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/index.html","id":"what-is-bayclump","dir":"","previous_headings":"","what":"What is BayClump?","title":"Bayesian Analysis of Clumped Isotope Datasets","text":"BayClump associated Shiny Dashboard application associated bayclumpr R package. functions implemented BayClump sourced bayclumpr. eveloped BayClump allow users less coding experience able access standarized resources calibrate derive reconstructions using clumped isotope datasets. can access BayClump directly .","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/index.html","id":"installing-bayclumpr","dir":"","previous_headings":"","what":"Installing bayclumpr","title":"Bayesian Analysis of Clumped Isotope Datasets","text":"latest version bayclumpr can installed directly GitHub repository using following lines code R","code":"library(devtools) install_github(\"Tripati-Lab/bayclumpr\")"},{"path":"https://tripati-lab.github.io/bayclumpr/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Bayesian Analysis of Clumped Isotope Datasets","text":"Please see contributing guide.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/index.html","id":"contact","dir":"","previous_headings":"","what":"Contact","title":"Bayesian Analysis of Clumped Isotope Datasets","text":"Please see package DESCRIPTION package authors.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.bayesian.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian regressions to calibrate the clumped isotopes paleothermometer using stan. — cal.bayesian","title":"Bayesian regressions to calibrate the clumped isotopes paleothermometer using stan. — cal.bayesian","text":"Bayesian regressions calibrate clumped isotopes paleothermometer using stan.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.bayesian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian regressions to calibrate the clumped isotopes paleothermometer using stan. — cal.bayesian","text":"","code":"cal.bayesian(   calibrationData,   numSavedSteps = 3000,   priors = \"Informative\",   MC = TRUE )"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.bayesian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian regressions to calibrate the clumped isotopes paleothermometer using stan. — cal.bayesian","text":"calibrationData target calibration dataset. numSavedSteps Number MCMC iterations save. priors Either Informative, Weak, Uninformative slope intercept. MC Multicore (TRUE/FALSE)","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ci.html","id":null,"dir":"Reference","previous_headings":"","what":"This function is used to generate CI estimates at given intervals. It is currently used for plotting in BayClump. — cal.ci","title":"This function is used to generate CI estimates at given intervals. It is currently used for plotting in BayClump. — cal.ci","text":"function used generate CI estimates given intervals. currently used plotting BayClump.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This function is used to generate CI estimates at given intervals. It is currently used for plotting in BayClump. — cal.ci","text":"","code":"cal.ci(data, from, to, length.out = 100)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This function is used to generate CI estimates at given intervals. It is currently used for plotting in BayClump. — cal.ci","text":"data data.frame two columns named beta alpha. result bootstrapping posterior distribution given calibration set. lower limit x. upper limit x. length.number breaks.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a synthetic dataset for clumped isotopes calibrations — cal.dataset","title":"Generate a synthetic dataset for clumped isotopes calibrations — cal.dataset","text":"Generate synthetic dataset clumped isotopes calibrations","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a synthetic dataset for clumped isotopes calibrations — cal.dataset","text":"","code":"cal.dataset(error = \"S1\", nobs = 1000)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a synthetic dataset for clumped isotopes calibrations — cal.dataset","text":"error Error scenario: low (S1), Intermediate (S2), High (S3) nobs Number observations simulated dataset","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.deming.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Deming regression models on a given calibration dataset — cal.deming","title":"Fit Deming regression models on a given calibration dataset — cal.deming","text":"Fit Deming regression models given calibration dataset","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.deming.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Deming regression models on a given calibration dataset — cal.deming","text":"","code":"cal.deming(data, replicates, samples = NULL)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.deming.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Deming regression models on a given calibration dataset — cal.deming","text":"data calibration dataset replicates Number bootstrap replicates samples Number samples per bootstrap replicate","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ion.bayesian.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian regressions to calibrate the clumped isotopes paleothermometer using stan. — cal.ion.bayesian","title":"Bayesian regressions to calibrate the clumped isotopes paleothermometer using stan. — cal.ion.bayesian","text":"Bayesian regressions calibrate clumped isotopes paleothermometer using stan.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ion.bayesian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian regressions to calibrate the clumped isotopes paleothermometer using stan. — cal.ion.bayesian","text":"","code":"cal.ion.bayesian(calibrationData, numSavedSteps = 3000, MC = TRUE)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ion.bayesian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian regressions to calibrate the clumped isotopes paleothermometer using stan. — cal.ion.bayesian","text":"calibrationData target calibration dataset. numSavedSteps Number MCMC iterations save. MC Multicore (TRUE/FALSE)","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ols.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit OLS regression models on a given calibration dataset — cal.ols","title":"Fit OLS regression models on a given calibration dataset — cal.ols","text":"Fit OLS regression models given calibration dataset","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit OLS regression models on a given calibration dataset — cal.ols","text":"","code":"cal.ols(data, replicates, samples = NULL)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.ols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit OLS regression models on a given calibration dataset — cal.ols","text":"data calibration dataset replicates Number bootstrap replicates samples Number samples per bootstrap replicate","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a dataset reflecting the priors used to run the analyses — cal.prior","title":"Generate a dataset reflecting the priors used to run the analyses — cal.prior","text":"Generate dataset reflecting priors used run analyses","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a dataset reflecting the priors used to run the analyses — cal.prior","text":"","code":"cal.prior(prior, n = 1000)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a dataset reflecting the priors used to run the analyses — cal.prior","text":"prior Informative n number observations simulate","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.wols.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit weighted OLS regression models on a given calibration dataset — cal.wols","title":"Fit weighted OLS regression models on a given calibration dataset — cal.wols","text":"Fit weighted OLS regression models given calibration dataset","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.wols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit weighted OLS regression models on a given calibration dataset — cal.wols","text":"","code":"cal.wols(data, replicates, samples = NULL)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.wols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit weighted OLS regression models on a given calibration dataset — cal.wols","text":"data calibration dataset replicates Number bootstrap replicates samples Number samples per bootstrap replicate","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.york.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit York regression models on a given calibration dataset — cal.york","title":"Fit York regression models on a given calibration dataset — cal.york","text":"Fit York regression models given calibration dataset","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.york.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit York regression models on a given calibration dataset — cal.york","text":"","code":"cal.york(data, replicates, samples = NULL)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/cal.york.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit York regression models on a given calibration dataset — cal.york","text":"data calibration dataset replicates Number bootstrap replicates samples Number samples per bootstrap replicate","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.bayesian.html","id":null,"dir":"Reference","previous_headings":"","what":"This function generate temperature predictions (in 10^6/T2) based on a calibration dataset and target D47. Note that this approach additionally accounts for measured error in the target D47. This approach is congruent with the one used in McClelland et al. (2022). — rec.bayesian","title":"This function generate temperature predictions (in 10^6/T2) based on a calibration dataset and target D47. Note that this approach additionally accounts for measured error in the target D47. This approach is congruent with the one used in McClelland et al. (2022). — rec.bayesian","text":"function generate temperature predictions (10^6/T2) based calibration dataset target D47. Note approach additionally accounts measured error target D47. approach congruent one used McClelland et al. (2022).","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.bayesian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This function generate temperature predictions (in 10^6/T2) based on a calibration dataset and target D47. Note that this approach additionally accounts for measured error in the target D47. This approach is congruent with the one used in McClelland et al. (2022). — rec.bayesian","text":"","code":"rec.bayesian(   calModel,   recData,   iter = 1000,   mixed = FALSE,   postcalsamples = NULL,   MC = TRUE )"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.bayesian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This function generate temperature predictions (in 10^6/T2) based on a calibration dataset and target D47. Note that this approach additionally accounts for measured error in the target D47. This approach is congruent with the one used in McClelland et al. (2022). — rec.bayesian","text":"calModel stan model analyzed. recData reconstruction dataset. iter Number replicates retain. mixed whether model calModel mixed . postcalsamples Number posterior samples analyze calibration step. MC Multicore (TRUE/FALSE)","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.clumped.html","id":null,"dir":"Reference","previous_headings":"","what":"This function performs temp reconstruction (10^6/T^2 with T in K) for multiple replicates of the same target. — rec.clumped","title":"This function performs temp reconstruction (10^6/T^2 with T in K) for multiple replicates of the same target. — rec.clumped","text":"function performs temp reconstruction (10^6/T^2 T K) multiple replicates target.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.clumped.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This function performs temp reconstruction (10^6/T^2 with T in K) for multiple replicates of the same target. — rec.clumped","text":"","code":"rec.clumped(recData, obCal)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.clumped.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This function performs temp reconstruction (10^6/T^2 with T in K) for multiple replicates of the same target. — rec.clumped","text":"recData Reconstruction dataset obCal data.frame summarizing distribution slopes intercepts","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.ion.bayesian.html","id":null,"dir":"Reference","previous_headings":"","what":"This function generates temperature predictions (in 10^6/T2) based on a calibration dataset and target D47. It includes measured error in the target D47 and ion as an additional predictor. — rec.ion.bayesian","title":"This function generates temperature predictions (in 10^6/T2) based on a calibration dataset and target D47. It includes measured error in the target D47 and ion as an additional predictor. — rec.ion.bayesian","text":"function generates temperature predictions (10^6/T2) based calibration dataset target D47. includes measured error target D47 ion additional predictor.","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.ion.bayesian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This function generates temperature predictions (in 10^6/T2) based on a calibration dataset and target D47. It includes measured error in the target D47 and ion as an additional predictor. — rec.ion.bayesian","text":"","code":"rec.ion.bayesian(   calModel,   recData,   iter = 1000,   postcalsamples = NULL,   MC = TRUE )"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.ion.bayesian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This function generates temperature predictions (in 10^6/T2) based on a calibration dataset and target D47. It includes measured error in the target D47 and ion as an additional predictor. — rec.ion.bayesian","text":"calModel fitted Stan model calibration. recData reconstruction dataset. iter Number sampling iterations. postcalsamples Number posterior calibration samples use. MC Enable multicore (default TRUE).","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a dataset reflecting the priors used to run the analyses — rec.prior","title":"Generate a dataset reflecting the priors used to run the analyses — rec.prior","text":"Generate dataset reflecting priors used run analyses","code":""},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a dataset reflecting the priors used to run the analyses — rec.prior","text":"","code":"rec.prior(prior, n = 1000)"},{"path":"https://tripati-lab.github.io/bayclumpr/reference/rec.prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a dataset reflecting the priors used to run the analyses — rec.prior","text":"prior Informative n number observations simulate","code":""}]
